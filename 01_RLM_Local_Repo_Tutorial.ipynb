{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Language Models (RLMs): Handling Infinite Context\n",
    "\n",
    "## Using the `rlm` Library\n",
    "\n",
    "---\n",
    "\n",
    "**Recursive Language Models (RLMs)** are a task-agnostic inference paradigm that enables language models to handle **near-infinite length contexts** by allowing the LM to *programmatically* examine, decompose, and recursively call itself over its input.\n",
    "\n",
    "### The Problem with Traditional LLMs\n",
    "\n",
    "Traditional LLMs have context window limitations:\n",
    "- GPT-4o: ~128K tokens\n",
    "- Claude: ~200K tokens\n",
    "- Even \"long context\" models struggle with millions of tokens\n",
    "\n",
    "When you have a document like **War and Peace** (~800K+ tokens), you either:\n",
    "1. Truncate it (losing information)\n",
    "2. Use RAG (which may miss important context)\n",
    "3. Use an RLM! \n",
    "\n",
    "### How RLMs Work\n",
    "\n",
    "RLMs replace the standard `llm.completion(prompt)` call with `rlm.completion(prompt)`:\n",
    "\n",
    "1. **Context as Environment Variable**: Instead of feeding the entire context to the LLM, RLM stores it as a variable (`context`) in a REPL environment\n",
    "2. **Programmatic Exploration**: The LLM writes Python code to examine, chunk, and analyze the context\n",
    "3. **Recursive Sub-Calls**: The LLM can call `llm_query()` to make sub-LLM calls on specific chunks\n",
    "4. **Iterative Refinement**: The process continues until the LLM produces a final answer\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [arXiv Paper](https://arxiv.org/abs/2512.24601) - Full technical details\n",
    "- [Blogpost](https://alexzhang13.github.io/blog/2025/rlm/) - Intuitive explanation\n",
    "- [Documentation](https://alexzhang13.github.io/rlm/) - API reference\n",
    "- [GitHub Repository](https://github.com/alexzhang13/rlm) - Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup Instructions\n",
    "\n",
    "### Clone the Repository\n",
    "\n",
    "First, clone the RLM repository. If you're running this notebook from the same directory as the cloned repo, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'rlm' already exists - using existing repo\n"
     ]
    }
   ],
   "source": [
    "# Clone the RLM repository (skip if 'rlm' directory already exists)\n",
    "# If you already have the repo cloned as 'rlm/', skip this cell\n",
    "!git clone https://github.com/alexzhang13/rlm.git rlm 2>/dev/null || echo \"Directory 'rlm' already exists - using existing repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "The `rlm` package requires **Python >= 3.11** and has the following core dependencies:\n",
    "- `openai>=2.14.0`\n",
    "- `anthropic>=0.75.0`\n",
    "- `google-genai>=1.56.0`\n",
    "- `rich>=13.0.0` (for beautiful console output)\n",
    "- `python-dotenv>=1.2.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m126 packages\u001b[0m \u001b[2min 38ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rlm\u001b[2m @ file:///home/chris/Code/AI%20Makerspace/Events/RLM-AIM-Event/rlm\u001b[\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rlm\u001b[2m @ file:///home/chris/Code/AI%20Makerspace/Events/RLM-AIM-Event/rlm\u001b[\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m rlm\u001b[2m @ file:///home/chris/Code/AI%20Makerspace/Events/RLM-AIM-Event/rlm\u001b[\n",
      "\u001b[2K\u001b[2A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m rlm\u001b[2m @ file:///home/chris/Code/AI%20Makerspace/Events/RLM-AIM-Event/rlm\u001b[\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 402ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.47ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.64ms\u001b[0m\u001b[0m///home/chris/Code/AI%20Makersp\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mrlm\u001b[0m\u001b[2m==0.1.0 (from file:///home/chris/Code/AI%20Makerspace/Events/RLM-AIM-Event/rlm)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the rlm package in editable mode\n",
    "# The 'rlm' directory should exist in the same folder as this notebook\n",
    "!uv pip install -e ./rlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RLM' from 'rlm' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Verify installation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrlm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RLM\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrlm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RLMLogger\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRLM imported successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'RLM' from 'rlm' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Verify installation\n",
    "from rlm import RLM\n",
    "from rlm.logger import RLMLogger\n",
    "\n",
    "print(\"RLM imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure API Keys\n",
    "\n",
    "RLM supports multiple LLM backends. You'll need API keys for the providers you want to use.\n",
    "\n",
    "**Option 1**: Create a `.env` file:\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "ANTHROPIC_API_KEY=sk-ant-...\n",
    "```\n",
    "\n",
    "**Option 2**: Set environment variables directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "\n",
    "# Verify at least one key is set\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API key configured\")\n",
    "if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    print(\"Anthropic API key configured\")\n",
    "    \n",
    "if not os.getenv(\"OPENAI_API_KEY\") and not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    print(\"WARNING: No API keys found! Please set OPENAI_API_KEY or ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Download Long-Context Dataset\n",
    "\n",
    "To demonstrate the power of RLMs, we'll use **War and Peace** by Leo Tolstoy - one of the longest novels ever written.\n",
    "\n",
    "- **Size**: ~3.3 million characters\n",
    "- **Approximate Tokens**: ~800,000+ tokens\n",
    "- **Source**: Project Gutenberg (public domain)\n",
    "\n",
    "This text is **far too long** for any traditional LLM context window, making it perfect for demonstrating RLM capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded War and Peace\n",
      "  Characters: 3,273,921\n",
      "  Approximate tokens: ~818,480\n",
      "  Lines: 65,650\n",
      "\n",
      "First 500 characters:\n",
      "--------------------------------------------------\n",
      "WAR AND PEACE\n",
      "\n",
      "\n",
      "By Leo Tolstoy/Tolstoi\n",
      "\n",
      "\n",
      "    Contents\n",
      "\n",
      "    BOOK ONE: 1805\n",
      "\n",
      "    CHAPTER I\n",
      "\n",
      "    CHAPTER II\n",
      "\n",
      "    CHAPTER III\n",
      "\n",
      "    CHAPTER IV\n",
      "\n",
      "    CHAPTER V\n",
      "\n",
      "    CHAPTER VI\n",
      "\n",
      "    CHAPTER VII\n",
      "\n",
      "    CHAPTER VIII\n",
      "\n",
      "    CHAPTER IX\n",
      "\n",
      "    CHAPTER X\n",
      "\n",
      "    CHAPTER XI\n",
      "\n",
      "    CHAPTER XII\n",
      "\n",
      "    CHAPTER XIII\n",
      "\n",
      "    CHAPTER XIV\n",
      "\n",
      "    CHAPTER XV\n",
      "\n",
      "    CHAPTER XVI\n",
      "\n",
      "    CHAPTER XVII\n",
      "\n",
      "    CHAPTER XVIII\n",
      "\n",
      "    CHAPTER XIX\n",
      "\n",
      "    CHAPTER XX\n",
      "\n",
      "    CHAPTER XXI\n",
      "\n",
      "    CHAPTER XXII\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download_gutenberg_text(url: str) -> str:\n",
    "    \"\"\"Download and clean a text file from Project Gutenberg.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    text = response.text\n",
    "    \n",
    "    # Remove Gutenberg header (find start of actual content)\n",
    "    start_markers = [\"*** START OF\", \"***START OF\"]\n",
    "    for marker in start_markers:\n",
    "        if marker in text:\n",
    "            text = text.split(marker, 1)[1]\n",
    "            text = text.split(\"\\n\", 1)[1]  # Skip the marker line\n",
    "            break\n",
    "    \n",
    "    # Remove Gutenberg footer\n",
    "    end_markers = [\"*** END OF\", \"***END OF\"]\n",
    "    for marker in end_markers:\n",
    "        if marker in text:\n",
    "            text = text.split(marker, 1)[0]\n",
    "            break\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Download War and Peace\n",
    "war_and_peace_url = \"https://www.gutenberg.org/files/2600/2600-0.txt\"\n",
    "war_and_peace = download_gutenberg_text(war_and_peace_url)\n",
    "\n",
    "print(f\"Downloaded War and Peace\")\n",
    "print(f\"  Characters: {len(war_and_peace):,}\")\n",
    "print(f\"  Approximate tokens: ~{len(war_and_peace) // 4:,}\")\n",
    "print(f\"  Lines: {len(war_and_peace.splitlines()):,}\")\n",
    "print()\n",
    "print(\"First 500 characters:\")\n",
    "print(\"-\" * 50)\n",
    "print(war_and_peace[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Basic RLM Usage with Verbose Logging\n",
    "\n",
    "Let's initialize an RLM instance and see it in action. We'll enable:\n",
    "- **`verbose=True`**: Rich console output showing each iteration\n",
    "- **`RLMLogger`**: JSON-lines file logging for detailed trajectory analysis\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `backend` | LLM provider: \"openai\", \"anthropic\", \"gemini\", \"portkey\", etc. | \"openai\" |\n",
    "| `backend_kwargs` | Provider-specific settings (model_name, api_key) | {} |\n",
    "| `environment` | Code execution: \"local\", \"docker\", \"modal\", \"prime\" | \"local\" |\n",
    "| `max_depth` | Recursion depth for sub-calls | 1 |\n",
    "| `max_iterations` | Maximum REPL iterations | 30 |\n",
    "| `verbose` | Enable rich console output | False |\n",
    "| `logger` | RLMLogger instance for file logging | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RLM' from 'rlm' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrlm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RLM\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrlm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RLMLogger\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create a logger to save trajectories\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'RLM' from 'rlm' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rlm import RLM\n",
    "from rlm.logger import RLMLogger\n",
    "\n",
    "# Create a logger to save trajectories\n",
    "logger = RLMLogger(log_dir=\"./logs\")\n",
    "\n",
    "# Initialize RLM with OpenAI backend\n",
    "rlm = RLM(\n",
    "    backend=\"openai\",\n",
    "    backend_kwargs={\n",
    "        \"model_name\": \"gpt-4o-mini\",  # Cost-effective for demos\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    },\n",
    "    environment=\"local\",  # Run code in local Python REPL\n",
    "    max_depth=1,          # Allow 1 level of recursive sub-calls\n",
    "    max_iterations=20,    # Maximum REPL iterations\n",
    "    logger=logger,        # Log to JSONL files\n",
    "    verbose=True,         # Show rich console output\n",
    ")\n",
    "\n",
    "print(\"RLM initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up: Simple Query\n",
    "\n",
    "Let's start with a simple query to see how RLM works. Watch the verbose output to see:\n",
    "1. The LLM generating Python code\n",
    "2. Code execution in the REPL\n",
    "3. The final answer extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple warm-up query\n",
    "result = rlm.completion(\"Print me the first 20 powers of two, each on a newline.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Response: {result.response}\")\n",
    "print(f\"\\nExecution Time: {result.execution_time:.2f}s\")\n",
    "print(f\"Model: {result.root_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Long-Context Challenge: Analyzing War and Peace\n",
    "\n",
    "Now let's tackle something that **no traditional LLM can handle**: analyzing the entire text of War and Peace!\n",
    "\n",
    "The RLM will:\n",
    "1. Store the entire 3.3M character text as a `context` variable\n",
    "2. Write Python code to chunk and analyze the text\n",
    "3. Use `llm_query()` to make sub-LLM calls on specific sections\n",
    "4. Synthesize findings into a final answer\n",
    "\n",
    "Watch the verbose output to see how the RLM approaches this massive text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query that requires understanding the entire text\n",
    "query = \"\"\"\n",
    "Count how many times the word \"war\" appears vs the word \"peace\" in this text.\n",
    "Then find 3 significant quotes that discuss the theme of war and 3 that discuss peace.\n",
    "Provide your analysis of what these word frequencies and quotes reveal about the novel's themes.\n",
    "\"\"\"\n",
    "\n",
    "# The context (War and Peace) is passed as the first argument\n",
    "# The query is passed as root_prompt\n",
    "result = rlm.completion(\n",
    "    war_and_peace,\n",
    "    root_prompt=query\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complex Query\n",
    "\n",
    "Let's try a query that requires deeper analysis across the entire novel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex analytical query\n",
    "complex_query = \"\"\"\n",
    "Analyze the structure of War and Peace:\n",
    "1. How many \"Books\" or major sections does it have?\n",
    "2. Identify the main characters mentioned most frequently in the first and last sections.\n",
    "3. How does the narrative focus shift from the beginning to the end?\n",
    "\n",
    "Provide specific evidence from the text to support your analysis.\n",
    "\"\"\"\n",
    "\n",
    "result = rlm.completion(\n",
    "    war_and_peace,\n",
    "    root_prompt=complex_query\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STRUCTURAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(result.response)\n",
    "print(f\"\\nExecution Time: {result.execution_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Examining Log Files\n",
    "\n",
    "The `RLMLogger` creates detailed JSON-lines files that capture every iteration of the RLM process. These are invaluable for:\n",
    "- Debugging unexpected behavior\n",
    "- Understanding how the RLM approaches problems\n",
    "- Analyzing token usage and cost\n",
    "\n",
    "Let's examine the log files we've created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find log files\n",
    "log_dir = Path(\"./logs\")\n",
    "if log_dir.exists():\n",
    "    log_files = sorted(log_dir.glob(\"*.jsonl\"), key=os.path.getmtime, reverse=True)\n",
    "    print(f\"Found {len(log_files)} log file(s):\\n\")\n",
    "    \n",
    "    for log_file in log_files[:3]:  # Show latest 3\n",
    "        print(f\"  {log_file.name}\")\n",
    "else:\n",
    "    print(\"No logs directory found. Run some RLM completions first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the most recent log file\n",
    "if log_dir.exists() and log_files:\n",
    "    latest_log = log_files[0]\n",
    "    print(f\"Analyzing: {latest_log.name}\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open(latest_log) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            if entry.get(\"type\") == \"metadata\":\n",
    "                print(\"METADATA:\")\n",
    "                print(f\"  Model: {entry.get('root_model')}\")\n",
    "                print(f\"  Max Iterations: {entry.get('max_iterations')}\")\n",
    "                print(f\"  Environment: {entry.get('environment')}\")\n",
    "                print()\n",
    "                \n",
    "            elif entry.get(\"type\") == \"iteration\":\n",
    "                iter_num = entry.get('iteration', i)\n",
    "                response_len = len(entry.get('response', ''))\n",
    "                code_blocks = entry.get('code_blocks', [])\n",
    "                final = entry.get('final_answer')\n",
    "                \n",
    "                print(f\"Iteration {iter_num}:\")\n",
    "                print(f\"  Response length: {response_len} chars\")\n",
    "                print(f\"  Code blocks: {len(code_blocks)}\")\n",
    "                if final:\n",
    "                    print(f\"  FINAL ANSWER: {final[:100]}...\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Using Different Backends\n",
    "\n",
    "The `rlm` library supports multiple LLM providers. Here's how to use different backends:\n",
    "\n",
    "### Available Backends\n",
    "- `\"openai\"` - OpenAI API (GPT-4o, GPT-4o-mini, etc.)\n",
    "- `\"anthropic\"` - Anthropic API (Claude models)\n",
    "- `\"gemini\"` - Google Gemini API\n",
    "- `\"portkey\"` - Portkey AI router\n",
    "- `\"litellm\"` - LiteLLM router\n",
    "- `\"azure_openai\"` - Azure OpenAI Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Anthropic (Claude)\n",
    "if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    rlm_anthropic = RLM(\n",
    "        backend=\"anthropic\",\n",
    "        backend_kwargs={\n",
    "            \"model_name\": \"claude-sonnet-4-20250514\",\n",
    "            \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            \"max_tokens\": 8192,\n",
    "        },\n",
    "        environment=\"local\",\n",
    "        max_depth=1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Test with a simple query\n",
    "    result = rlm_anthropic.completion(\"Calculate the factorial of 10 step by step.\")\n",
    "    print(f\"\\nClaude's answer: {result.response}\")\n",
    "else:\n",
    "    print(\"Set ANTHROPIC_API_KEY to use Claude models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Model Configuration\n",
    "\n",
    "You can use different models for the root reasoning vs. sub-calls. This is useful for cost optimization - use a powerful model for reasoning and a cheaper model for sub-queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-model setup: GPT-4o for root, GPT-4o-mini for sub-calls\n",
    "rlm_multi = RLM(\n",
    "    backend=\"openai\",\n",
    "    backend_kwargs={\n",
    "        \"model_name\": \"gpt-4o\",  # Main reasoning model\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    },\n",
    "    other_backends=[\"openai\"],  # Additional backends for sub-calls\n",
    "    other_backend_kwargs=[{\n",
    "        \"model_name\": \"gpt-4o-mini\",  # Cheaper model for sub-queries\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    }],\n",
    "    environment=\"local\",\n",
    "    max_depth=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Multi-model RLM configured!\")\n",
    "print(\"  Root model: gpt-4o (for main reasoning)\")\n",
    "print(\"  Sub-call model: gpt-4o-mini (for chunked queries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Understanding RLM Internals\n",
    "\n",
    "### The REPL Environment\n",
    "\n",
    "When you call `rlm.completion(context, root_prompt=query)`, the RLM:\n",
    "\n",
    "1. **Stores context as a variable**: `context = \"<your text>\"`\n",
    "2. **Provides special functions**:\n",
    "   - `llm_query(prompt)` - Make a sub-LLM call\n",
    "   - `llm_query_batched(prompts)` - Make multiple concurrent sub-calls\n",
    "   - `FINAL(answer)` - Return the final answer\n",
    "   - `FINAL_VAR(variable_name)` - Return a variable as the answer\n",
    "\n",
    "3. **Iterates**: The LLM generates code, executes it, sees results, and continues\n",
    "\n",
    "### System Prompt\n",
    "\n",
    "The RLM uses a carefully crafted system prompt that teaches the LLM how to:\n",
    "- Access and manipulate the `context` variable\n",
    "- Use chunking strategies for large texts\n",
    "- Make recursive sub-LLM calls\n",
    "- Format final answers correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a query that demonstrates sub-LLM calls\n",
    "sub_call_query = \"\"\"\n",
    "Find the first chapter of War and Peace. Then use llm_query to:\n",
    "1. Summarize that chapter in 2 sentences\n",
    "2. Identify the main characters introduced\n",
    "\n",
    "Combine these into your final answer.\n",
    "\"\"\"\n",
    "\n",
    "result = rlm.completion(\n",
    "    war_and_peace[:100000],  # First ~100K chars for faster demo\n",
    "    root_prompt=sub_call_query\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS WITH SUB-CALLS\")\n",
    "print(\"=\" * 60)\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Token Usage and Cost Tracking\n",
    "\n",
    "The `RLMChatCompletion` object includes detailed usage statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a query and examine usage\n",
    "result = rlm.completion(\n",
    "    \"The secret codes are: ALPHA=42, BETA=17, GAMMA=99. Remember these.\",\n",
    "    root_prompt=\"What is the sum of ALPHA, BETA, and GAMMA?\"\n",
    ")\n",
    "\n",
    "print(\"Result:\", result.response)\n",
    "print(\"\\nUsage Summary:\")\n",
    "print(f\"  Execution time: {result.execution_time:.2f}s\")\n",
    "print(f\"  Model: {result.root_model}\")\n",
    "\n",
    "usage = result.usage_summary\n",
    "if usage:\n",
    "    usage_dict = usage.to_dict()\n",
    "    print(f\"  Token usage: {usage_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Visualizing Trajectories\n",
    "\n",
    "The `rlm` repository includes a **visualizer tool** for exploring RLM trajectories interactively.\n",
    "\n",
    "To use it:\n",
    "\n",
    "```bash\n",
    "cd rlm/visualizer  # or rlm_repo/visualizer\n",
    "npm install\n",
    "npm run dev        # Opens on localhost:3001\n",
    "```\n",
    "\n",
    "You can then load the `.jsonl` log files and explore:\n",
    "- Each iteration's LLM response\n",
    "- Code executed and output\n",
    "- Sub-LLM calls and their results\n",
    "- Token usage per step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Conclusion\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **RLMs solve the context length problem** by treating context as a variable, not input\n",
    "2. **Programmatic exploration** lets LLMs write code to analyze massive texts\n",
    "3. **Recursive sub-calls** enable divide-and-conquer strategies\n",
    "4. **Verbose logging** provides transparency into the reasoning process\n",
    "\n",
    "### When to Use RLMs\n",
    "\n",
    "- Documents too large for any context window (books, codebases, logs)\n",
    "- Tasks requiring systematic analysis (counting, searching, comparing)\n",
    "- Queries needing evidence from multiple parts of a document\n",
    "- When RAG might miss important context\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try the **Docker environment** for isolated code execution\n",
    "- Explore **Modal** or **Prime** for cloud-based sandboxes\n",
    "- Check out the **visualizer** for trajectory analysis\n",
    "- Read the [full paper](https://arxiv.org/abs/2512.24601) for technical details\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [GitHub Repository](https://github.com/alexzhang13/rlm)\n",
    "- [Documentation](https://alexzhang13.github.io/rlm/)\n",
    "- [arXiv Paper](https://arxiv.org/abs/2512.24601)\n",
    "- [Blogpost](https://alexzhang13.github.io/blog/2025/rlm/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
